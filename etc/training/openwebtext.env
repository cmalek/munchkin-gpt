# Dataset
# -------
DATASET=openwebtext
VAL_SIZE=0.005

# Training
# --------
NAME=openwebtext
BATCH_SIZE=12
EVAL_INTERVAL=5000
MAX_ITERS=600000
WARMUP_ITERS=100  # not super necessary potentially
LR_DECAY_ITERS=600000  # make equal to max_iters usually
GRADIENT_ACCUMULATION_STEPS=40
ALWAYS_SAVE_CHECKPOINT=False

# Logging
# -------
LOG_INTERVAL=10  # don't print too too often
EVAL_ITERS=200
WANDB_LOG=False  # override via command line if you like
WANDB_PROJECT=shakespeare-char
WANDB_RUN_NAME=nanogpt


# Model
# -----
BLOCK_SIZE=1024
N_LAYER=12
N_HEAD=12
N_EMBD=768
DROPOUT=0.2

# Torch
# -----
DEVICE=mps  # run on mps on Macs only
COMPILE=True # do not torch compile the model