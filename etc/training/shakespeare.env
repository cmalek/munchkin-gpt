# Dataset
# -------
DATASET=shakespeare
VAL_SIZE=0.1

# Training
# --------
NAME=shakespeare
BATCH_SIZE=64
EVAL_INTERVAL=250  # keep frequent because we'll overfit
MAX_ITERS=5000
LEARNING_RATE=1e-3  # with baby networks can afford to go a bit higher
WARMUP_ITERS=100  # not super necessary potentially
LR_DECAY_ITERS=5000  # make equal to max_iters usually
GRADIENT_ACCUMULATION_STEPS=1
MIN_LR=1e-4  # learning_rate / 10 usually
BETA2=0.99  # make a bit bigger because number of tokens per iter is small
# we expect to overfit on this small dataset, so only save when val improves
ALWAYS_SAVE_CHECKPOINT=False

# Logging
# -------
LOG_INTERVAL=10  # don't print too too often
EVAL_ITERS=200
WANDB_LOG=False  # override via command line if you like
WANDB_PROJECT=shakespeare-char
WANDB_RUN_NAME=nanogpt


# Model
# -----
BLOCK_SIZE=256  # context of up to 256 previous characters
N_LAYER=6
N_HEAD=6
N_EMBD=384
DROPOUT=0.2

# Torch
# -----
DEVICE=mps  # run on mps on Macs only
COMPILE=False # do not torch compile the model